Semantic Vector Search Engine

A modular semantic search system built using Python, FastAPI, and Sentence-Transformers.
This project demonstrates:

Generating embeddings for text documents

Caching embeddings to avoid recomputation

Performing vector search using cosine similarity

Exposing a search API for retrieval

Providing ranking explanations for results

ğŸ“ Project Structure
project/
â”‚
â”œâ”€â”€ data/               # ignored in Git
â”‚   â””â”€â”€ docs/           # dataset text files
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ embedder.py
â”‚   â”œâ”€â”€ embedding_model.py
â”‚   â”œâ”€â”€ cache_manager.py
â”‚   â”œâ”€â”€ search_engine.py
â”‚   â”œâ”€â”€ api.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore

âš¡ How Caching Works

We use a simple JSON file (embedding_cache.json) to store embeddings.

Each entry looks like:

{
  "doc_id": "doc_001",
  "embedding": [...384-dimensional vector...],
  "hash": "sha256_text_hash",
  "updated_at": 1732138217.221
}

Caching Logic

Read document

Clean text (lowercase, remove HTML, trim spaces)

Compute SHA-256 hash

Compare hash with cached version

If same â†’ load cached embedding

If different â†’ compute new embedding and update cache

This makes search extremely fast since embeddings don't need to be recomputed.

ğŸ“¥ Downloading the Dataset

Use the provided script:

python download_dataset.py


This creates:

data/docs/doc_0000.txt ... doc_0199.txt


(These files are ignored in GitHub via .gitignore)

ğŸ§  Embedding Pipeline

To test embedding generation:

cd src
python test_embedding.py


You should see logs like:

Loading embedding model: all-MiniLM-L6-v2
Embedding shape: (384,)

ğŸš€ Starting the Search API

Run the FastAPI server:

cd src
uvicorn api:app --reload --host 127.0.0.1 --port 8000


API docs open at:

ğŸ‘‰ http://127.0.0.1:8000/docs

ğŸ” Running a Search Query

Example request:

POST /search


Body:

{
  "query": "machine learning algorithms",
  "top_k": 5
}


Response:

{
  "results": [
    ["doc_0153", 0.42],
    ["doc_0059", 0.40],
    ["doc_0049", 0.28]
  ]
}

ğŸ“Š Ranking Explanation

Each search result includes (if enabled):

doc_id â€“ which document matched

score â€“ cosine similarity

reason â€“ simple keyword overlap check

overlap_ratio â€“ heuristic scoring

length_norm â€“ optional length-normalized score

This helps understand why the model picked a document.

ğŸ—ï¸ Design Choices
Embedding Model

sentence-transformers/all-MiniLM-L6-v2

Small, fast, accurate for semantic vector search

Cache

Simple JSON-based storage

Easy to inspect, portable, reliable

Search Engine

Pure NumPy cosine similarity

Simple and transparent

API

FastAPI for clean automatic documentation

Pydantic validation

ğŸ“¦ Installation
pip install -r requirements.txt

â–¶ï¸ Full Run Instructions
python download_dataset.py
cd src
python test_embedding.py
uvicorn api:app --reload


Then visit:

ğŸ‘‰ http://127.0.0.1:8000/docs

ğŸ¯ Optional Improvements (Bonus Ideas)

Streamlit UI

FAISS index

Query expansion using WordNet

Multiprocessing for batch embeddings

Quality evaluation with test queries